{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, remove_columns=[\"text\"], batched=True\n",
    ")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(CustomBERT, self).__init__()\n",
    "\n",
    "        # Load pretrained BERT model for sequence classification\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Custom layer before classification head\n",
    "        self.custom_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        # Custom BERT classifier head\n",
    "        self.bert.classifier = nn.Linear(in_features=1024, out_features=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        # BERT output(ignoring classification head)\n",
    "        bert_output = self.bert.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract last hidden state\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "        # Pass the representation of [CLS] token through custom layer\n",
    "        custom_layer_output = self.custom_layer(last_hidden_state[:, 0, :])\n",
    "\n",
    "        # Get the logits from the final classifier\n",
    "        logits = self.bert.classifier(custom_layer_output)\n",
    "\n",
    "        # Compute loss and backpropagation\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_func = nn.CrossEntropyLoss()  # Loss function\n",
    "            loss = loss_func(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CustomBERT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_steps=25,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].select(range(1000)),\n",
    "    eval_dataset=tokenized_dataset[\"test\"].select(range(1000)),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# trainer.train()\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "# \"resume_from_checkpoint=True\" only works when output_dir already has saved chechpoint. Else throws error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize input text and move it to the device (GPU/CPU)\n",
    "    input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(\n",
    "            input_ids=input[\"input_ids\"],\n",
    "            attention_mask=input[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    # Extract logits from output\n",
    "    logits = output[\"logits\"]\n",
    "\n",
    "    # Apply sigmoid to get probabilities and use argmax to get the predicted class\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    # Assign sentiment based on the predicted class\n",
    "    sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "text = \"The movie was good.\"\n",
    "sentiment = predict(text)\n",
    "print(f\"The sentiment of the text is: {sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
